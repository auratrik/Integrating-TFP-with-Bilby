{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "%matplotlib notebook\n",
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import bilby\n",
    "from bilby.core.prior import Uniform\n",
    "from bilby.gw.conversion import convert_to_lal_binary_black_hole_parameters, generate_all_bbh_parameters\n",
    "\n",
    "from gwpy.timeseries import TimeSeries\n",
    "\n",
    "import lal\n",
    "import lalsimulation as lalsim\n",
    "\n",
    "from bilby.core import utils\n",
    "from bilby.core.utils import logger\n",
    "from bilby.gw.conversion import bilby_to_lalsimulation_spins\n",
    "from bilby.gw.utils import (lalsim_GetApproximantFromString,\n",
    "                    lalsim_SimInspiralFD,\n",
    "                    lalsim_SimInspiralChooseFDWaveform,\n",
    "                    lalsim_SimInspiralWaveformParamsInsertTidalLambda1,\n",
    "                    lalsim_SimInspiralWaveformParamsInsertTidalLambda2,\n",
    "                    lalsim_SimInspiralChooseFDWaveformSequence,\n",
    "                    convert_args_list_to_float, \n",
    "                    _get_lalsim_approximant)\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numbers\n",
    "from itertools import chain, combinations\n",
    "from itertools import combinations_with_replacement as combinations_w_r\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.interpolate import BSpline\n",
    "from scipy.special import comb\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils.deprecation import deprecated\n",
    "from sklearn.utils.validation import check_is_fitted, FLOAT_DTYPES, _check_sample_weight\n",
    "from sklearn.utils.validation import _check_feature_names_in\n",
    "from sklearn.utils.stats import _weighted_percentile\n",
    "\n",
    "#from ._csr_polynomial_expansion import _csr_polynomial_expansion\n",
    "\n",
    "\n",
    "#__all__ = [\n",
    "#    \"PolynomialFeatures\",\n",
    "#    \"SplineTransformer\",\n",
    "#]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(self, X):\n",
    "        \"\"\"Transform data to polynomial features.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The data to transform, row by row.\n",
    "            Prefer CSR over CSC for sparse input (for speed), but CSC is\n",
    "            required if the degree is 4 or higher. If the degree is less than\n",
    "            4 and the input format is CSC, it will be converted to CSR, have\n",
    "            its polynomial features generated, then converted back to CSC.\n",
    "            If the degree is 2 or 3, the method described in \"Leveraging\n",
    "            Sparsity to Speed Up Polynomial Feature Expansions of CSR Matrices\n",
    "            Using K-Simplex Numbers\" by Andrew Nystrom and John Hughes is\n",
    "            used, which is much faster than the method used on CSC input. For\n",
    "            this reason, a CSC input will be converted to CSR, and the output\n",
    "            will be converted back to CSC prior to being returned, hence the\n",
    "            preference of CSR.\n",
    "        Returns\n",
    "        -------\n",
    "        XP : {ndarray, sparse matrix} of shape (n_samples, NP)\n",
    "            The matrix of features, where `NP` is the number of polynomial\n",
    "            features generated from the combination of inputs. If a sparse\n",
    "            matrix is provided, it will be converted into a sparse\n",
    "            `csr_matrix`.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        X = self._validate_data(\n",
    "            X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
    "        )\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if sparse.isspmatrix_csr(X):\n",
    "            if self._max_degree > 3:\n",
    "                return self.transform(X.tocsc()).tocsr()\n",
    "            to_stack = []\n",
    "            if self.include_bias:\n",
    "                to_stack.append(\n",
    "                    sparse.csc_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
    "                )\n",
    "            if self._min_degree <= 1 and self._max_degree > 0:\n",
    "                to_stack.append(X)\n",
    "            for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
    "                Xp_next = _csr_polynomial_expansion(\n",
    "                    X.data, X.indices, X.indptr, X.shape[1], self.interaction_only, deg\n",
    "                )\n",
    "                if Xp_next is None:\n",
    "                    break\n",
    "                to_stack.append(Xp_next)\n",
    "            if len(to_stack) == 0:\n",
    "                # edge case: deal with empty matrix\n",
    "                XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
    "            else:\n",
    "                XP = sparse.hstack(to_stack, format=\"csr\")\n",
    "        elif sparse.isspmatrix_csc(X) and self._max_degree < 4:\n",
    "            return self.transform(X.tocsr()).tocsc()\n",
    "        elif sparse.isspmatrix(X):\n",
    "            combinations = self._combinations(\n",
    "                n_features=n_features,\n",
    "                min_degree=self._min_degree,\n",
    "                max_degree=self._max_degree,\n",
    "                interaction_only=self.interaction_only,\n",
    "                include_bias=self.include_bias,\n",
    "            )\n",
    "            columns = []\n",
    "            for combi in combinations:\n",
    "                if combi:\n",
    "                    out_col = 1\n",
    "                    for col_idx in combi:\n",
    "                        out_col = X[:, col_idx].multiply(out_col)\n",
    "                    columns.append(out_col)\n",
    "                else:\n",
    "                    bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
    "                    columns.append(bias)\n",
    "            XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
    "        else:\n",
    "            # Do as if _min_degree = 0 and cut down array after the\n",
    "            # computation, i.e. use _n_out_full instead of n_output_features_.\n",
    "            XP = jnp.empty(\n",
    "                shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
    "            )\n",
    "\n",
    "            # What follows is a faster implementation of:\n",
    "            # for i, comb in enumerate(combinations):\n",
    "            #     XP[:, i] = X[:, comb].prod(1)\n",
    "            # This implementation uses two optimisations.\n",
    "            # First one is broadcasting,\n",
    "            # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
    "            # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
    "            # ...\n",
    "            # multiply ([X[:, start:end], X[:, start]) -> ...\n",
    "            # Second optimisation happens for degrees >= 3.\n",
    "            # Xi^3 is computed reusing previous computation:\n",
    "            # Xi^3 = Xi^2 * Xi.\n",
    "\n",
    "            # degree 0 term\n",
    "            if self.include_bias:\n",
    "                XP[:, 0] = 1\n",
    "                current_col = 1\n",
    "            else:\n",
    "                current_col = 0\n",
    "\n",
    "            if self._max_degree == 0:\n",
    "                return XP\n",
    "\n",
    "            # degree 1 term\n",
    "            XP[:, current_col : current_col + n_features] = X\n",
    "            index = list(range(current_col, current_col + n_features))\n",
    "            current_col += n_features\n",
    "            index.append(current_col)\n",
    "\n",
    "            # loop over degree >= 2 terms\n",
    "            for _ in range(2, self._max_degree + 1):\n",
    "                new_index = []\n",
    "                end = index[-1]\n",
    "                for feature_idx in range(n_features):\n",
    "                    start = index[feature_idx]\n",
    "                    new_index.append(current_col)\n",
    "                    if self.interaction_only:\n",
    "                        start += index[feature_idx + 1] - index[feature_idx]\n",
    "                    next_col = current_col + end - start\n",
    "                    if next_col <= current_col:\n",
    "                        break\n",
    "                    # XP[:, start:end] are terms of degree d - 1\n",
    "                    # that exclude feature #feature_idx.\n",
    "                    jnp.multiply(\n",
    "                        XP[:, start:end],\n",
    "                        X[:, feature_idx : feature_idx + 1],\n",
    "                        out=XP[:, current_col:next_col],\n",
    "                        casting=\"no\",\n",
    "                    )\n",
    "                    current_col = next_col\n",
    "\n",
    "                new_index.append(current_col)\n",
    "                index = new_index\n",
    "\n",
    "            if self._min_degree > 1:\n",
    "                n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
    "                if self.include_bias:\n",
    "                    Xout = jnp.empty(\n",
    "                        shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
    "                    )\n",
    "                    Xout[:, 0] = 1\n",
    "                    Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
    "                else:\n",
    "                    Xout = XP[:, n_XP - n_Xout :].copy()\n",
    "                XP = Xout\n",
    "        return XP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ones((5,5))\n",
    "Y = np.ones((5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XP' = transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform2(X):\n",
    "        #check_is_fitted(self)\n",
    "\n",
    "        #X = self._validate_data(\n",
    "        #    X, order=\"F\", dtype=FLOAT_DTYPES, reset=False, accept_sparse=(\"csr\", \"csc\")\n",
    "        #)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if sparse.isspmatrix_csr(X) and False:\n",
    "            if self._max_degree > 3:\n",
    "                return self.transform(X.tocsc()).tocsr()\n",
    "            to_stack = []\n",
    "            if self.include_bias:\n",
    "                to_stack.append(\n",
    "                    sparse.csc_matrix(np.ones(shape=(n_samples, 1), dtype=X.dtype))\n",
    "                )\n",
    "            if self._min_degree <= 1 and self._max_degree > 0:\n",
    "                to_stack.append(X)\n",
    "            for deg in range(max(2, self._min_degree), self._max_degree + 1):\n",
    "                Xp_next = _csr_polynomial_expansion(\n",
    "                    X.data, X.indices, X.indptr, X.shape[1], self.interaction_only, deg\n",
    "                )\n",
    "                if Xp_next is None:\n",
    "                    break\n",
    "                to_stack.append(Xp_next)\n",
    "            if len(to_stack) == 0:\n",
    "                # edge case: deal with empty matrix\n",
    "                XP = sparse.csr_matrix((n_samples, 0), dtype=X.dtype)\n",
    "            else:\n",
    "                XP = sparse.hstack(to_stack, format=\"csr\")\n",
    "        elif sparse.isspmatrix_csc(X) and self._max_degree < 4 and False: \n",
    "            return self.transform(X.tocsr()).tocsc()\n",
    "        elif sparse.isspmatrix(X) and False:\n",
    "            combinations = self._combinations(\n",
    "                n_features=n_features,\n",
    "                min_degree=self._min_degree,\n",
    "                max_degree=self._max_degree,\n",
    "                interaction_only=self.interaction_only,\n",
    "                include_bias=self.include_bias,\n",
    "            )\n",
    "            columns = []\n",
    "            for combi in combinations:\n",
    "                if combi:\n",
    "                    out_col = 1\n",
    "                    for col_idx in combi:\n",
    "                        out_col = X[:, col_idx].multiply(out_col)\n",
    "                    columns.append(out_col)\n",
    "                else:\n",
    "                    bias = sparse.csc_matrix(np.ones((X.shape[0], 1)))\n",
    "                    columns.append(bias)\n",
    "            XP = sparse.hstack(columns, dtype=X.dtype).tocsc()\n",
    "        else:\n",
    "            # Do as if _min_degree = 0 and cut down array after the\n",
    "            # computation, i.e. use _n_out_full instead of n_output_features_.\n",
    "            XP = jnp.empty(\n",
    "                shape=(n_samples, self._n_out_full), dtype=X.dtype, order=self.order\n",
    "            )\n",
    "\n",
    "            # What follows is a faster implementation of:\n",
    "            # for i, comb in enumerate(combinations):\n",
    "            #     XP[:, i] = X[:, comb].prod(1)\n",
    "            # This implementation uses two optimisations.\n",
    "            # First one is broadcasting,\n",
    "            # multiply ([X1, ..., Xn], X1) -> [X1 X1, ..., Xn X1]\n",
    "            # multiply ([X2, ..., Xn], X2) -> [X2 X2, ..., Xn X2]\n",
    "            # ...\n",
    "            # multiply ([X[:, start:end], X[:, start]) -> ...\n",
    "            # Second optimisation happens for degrees >= 3.\n",
    "            # Xi^3 is computed reusing previous computation:\n",
    "            # Xi^3 = Xi^2 * Xi.\n",
    "\n",
    "            # degree 0 term\n",
    "            if self.include_bias and False:\n",
    "                XP[:, 0] = 1\n",
    "                current_col = 1\n",
    "            else:\n",
    "                current_col = 0\n",
    "\n",
    "            #if self._max_degree == 0:\n",
    "            #    return XP\n",
    "\n",
    "            # degree 1 term\n",
    "            XP[:, current_col : current_col + n_features] = X\n",
    "            index = list(range(current_col, current_col + n_features))\n",
    "            current_col += n_features\n",
    "            index.append(current_col)\n",
    "\n",
    "            # loop over degree >= 2 terms\n",
    "            for _ in range(2, self._max_degree + 1):\n",
    "                new_index = []\n",
    "                end = index[-1]\n",
    "                for feature_idx in range(n_features):\n",
    "                    start = index[feature_idx]\n",
    "                    new_index.append(current_col)\n",
    "                    if self.interaction_only:\n",
    "                        start += index[feature_idx + 1] - index[feature_idx]\n",
    "                    next_col = current_col + end - start\n",
    "                    if next_col <= current_col:\n",
    "                        break\n",
    "                    # XP[:, start:end] are terms of degree d - 1\n",
    "                    # that exclude feature #feature_idx.\n",
    "                    jnp.multiply(\n",
    "                        XP[:, start:end],\n",
    "                        X[:, feature_idx : feature_idx + 1],\n",
    "                        out=XP[:, current_col:next_col],\n",
    "                        casting=\"no\",\n",
    "                    )\n",
    "                    current_col = next_col\n",
    "\n",
    "                new_index.append(current_col)\n",
    "                index = new_index\n",
    "\n",
    "            if self._min_degree > 1 and False:\n",
    "                n_XP, n_Xout = self._n_out_full, self.n_output_features_\n",
    "                if self.include_bias:\n",
    "                    Xout = jnp.empty(\n",
    "                        shape=(n_samples, n_Xout), dtype=XP.dtype, order=self.order\n",
    "                    )\n",
    "                    Xout[:, 0] = 1\n",
    "                    Xout[:, 1:] = XP[:, n_XP - n_Xout + 1 :]\n",
    "                else:\n",
    "                    Xout = XP[:, n_XP - n_Xout :].copy()\n",
    "                XP = Xout\n",
    "        return XP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ones((5,5))\n",
    "Y = np.ones((5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m XP \u001b[38;5;241m=\u001b[39m \u001b[43mtransform2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtransform2\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     52\u001b[0m     XP \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mhstack(columns, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mtocsc()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Do as if _min_degree = 0 and cut down array after the\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# computation, i.e. use _n_out_full instead of n_output_features_.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     XP \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mempty(\n\u001b[0;32m---> 57\u001b[0m         shape\u001b[38;5;241m=\u001b[39m(n_samples, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39m_n_out_full), dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morder\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# What follows is a faster implementation of:\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# for i, comb in enumerate(combinations):\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m#     XP[:, i] = X[:, comb].prod(1)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# degree 0 term\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_bias \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "XP = transform2(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python igwn",
   "language": "python",
   "name": "igwn-py39-lw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
